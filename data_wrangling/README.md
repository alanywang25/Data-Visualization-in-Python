## Data Merging and Wrangling Workflow
This is a more detailed description of our workflow for working with the data to create our visualizations. For the purposes of data privacy, we've used test synthetic data for these demos and the visualization demos, generated by subsetting 100 or data observations and creating fake usernames to replace the 'author' column in some of our datasets. The code and more details for the aforementioned data manipulations that we did can be found in the notebooks of this folder, listed in the same order as they are below.

### Combining Multiple Files of the Same Type
We first combined all of the `.tsv` files for our first dataset so that we could work with all of the dataset in one dataframe. We then saved that combined dataframe as a `.csv` file. 

### Converting Unix Time to Human-Readable and Timezone-Adjusted Time 
In our datasets, the posting times and location reveal times were in Unix Time format, so we had to implement methods from the `pytz`, `pandas`, and `datetime` Python libraries to convert all of the times into more readable times and extract out the converted hour of day and day of the week. 

### Merging Datasets
We also initially were not sure how to integrate the separate datasets together because they were provided to us separately, but we were able to merge parts of the datasets together using the `.merge()` function from the `pandas` dataframe by the common variables present in one or more of the datasets we wanted to merge. Once we were able to merge these datasets, we found that we could create much more insightful visualizations. 

### Subsetting Large Datasets for Data Manipulations
We also wanted to run time conversions on our entire combined dataset after merging it with the geographic information dataset, but because our combined dataset is so large, we ran the time conversion code in sections. After doing the time conversion on all of the small groups, we merged the subsetted dataframes into one big dataframe and saved that dataframe as a parquet file. For all of the data manipulations and data merging that we did, we found that saving our modified dataframes in the `.parquet` file format was very efficient.

### Dropping N/A and Duplicate Values for Non-GPU Visualizations
Because of the limitations of the `Python 3` kernel, we needed to drop any observations that contained `NaN` for some variables or any duplicate observations for one of our visualizations so that the visualization would be able to load with the geoviews library, which is not supported in the `rapids` kernel for GPU visualization. This, in turn, helped prevent overplotting while still being representative of the entirety of the dataset.

### Data Preparation for GPU Visualization
For our GPU visualization specifically, we found that we needed to convert our longitude/latitude points into Mercator points so that our data points would properly load onto our map. We also needed to map our day of the week variable to a set of numbers to create the interactive dropdown menu for our visualization. In addition, to increase efficiency, before converting our original `pandas` dataframe to a `cudf` dataframe for the GPU visualization to load, we extracted only the columns we wanted in our dataframe.

### Data Preparation for Choropleth Map Visualizations
To produce the US choropleth visualizations we first loaded the geolocation (country/city) file and renamed the columns for convenience. Then we loaded our combined dataset that we created earlier, dropped duplicate authors, and only selected the columns that were essential for our visualization. We then merged both dataframes, calculated the post frequency of each US state and finally used a dictionary to add a column with state abbreviations (e.g 'Florida': 'FL'). With this final dataframe we were able to create the US post frequency choropleth map.

We then kept on wrangling the data to create visualizations for the US with the demographic information. We started by loading the demographic information, renamed the `'name'` column to `'subreddit'`, and selected only the useful columns for this visualization. Then we merged this dataframe with the one we used to create the US state frequency visualization. In addition, we created a separate dataframe calculating the mean affluence percentile of every state, which we then merged onto the previous dataframe. We made sure to fill any `NaN` values with the proper state abbreviation and then created the visualization.
