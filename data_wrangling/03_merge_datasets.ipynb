{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aeebaaf-bf38-4b37-b59b-53729b72695f",
   "metadata": {},
   "source": [
    "# Merging Datasets\n",
    "\n",
    "To create more insightful visualizations, it may be helpful to merge two or more datasets by common variables using the `pandas` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc02340d-2dbc-4422-8275-943b63707402",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Here we are importing the necessary libraries to run the merging code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b0b2d65-9498-40f3-b0fb-203523330473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f9d18-faad-4956-a11e-8ff44d497bfb",
   "metadata": {},
   "source": [
    "## Read in Datasets\n",
    "\n",
    "Read in the datasets that you want to merge together. The `os.getcwd()` method gets the current working directory that you are in, which should be inside the `data_wrangling` folder. However, to access the data, we need to replace the current working directory with the directory that leads to the data files. Once that has been done, we can go head with reading in the data and performing the necessary data manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab155749-9b2d-40d9-82df-537042df7039",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.getcwd()\n",
    "DATA_DIR = DATA_DIR.replace('data_wrangling', 'synthetic_data')\n",
    "\n",
    "combined_df = pd.read_parquet(DATA_DIR + '/combined_posts_file.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bac10f3b-0c9a-478b-8567-20a0f1c3b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_data = pd.read_parquet(DATA_DIR + '/geo_known_synthetic.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c448a868-29c5-4063-892c-ca4abb0a4c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subred_demo = pd.read_csv(DATA_DIR + '/socdimclusters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fcd659-280b-40c6-94fa-8ece6c1e6d1d",
   "metadata": {},
   "source": [
    "## Extract/Rename Desired Columns\n",
    "\n",
    "To save space and increase efficiency, you can take only the columns that you want from each dataset so that the entirety of the datasets are not merged. You can also rename certain columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aea849a5-9371-4295-8373-89479b9d6c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timezone_loc = location_data[[\"author_synthetic\", \"timezone\", \"created_utc\", \"long\", \"lat\"]]\n",
    "timezone_loc = timezone_loc.rename(columns={'created_utc': 'loc_reveal_time'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a23aef5-53e4-4d45-a37c-047313c230ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "subred_cluster = subred_demo[['name', 'cluster_name']]\n",
    "subred_cluster = subred_cluster.rename(columns={'name': 'subreddit'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234ab08a-f54c-4e4f-bd8c-68178b380980",
   "metadata": {},
   "source": [
    "## Merge Datasets\n",
    "\n",
    "Now that the data has been prepared for merging, use the `.merge()` method from the pandas library to combine the datasets together. For more information, you can visit this site: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83aad6ab-a65c-4e82-8b19-fdc5996b1050",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['subreddit'] = combined_df['subreddit'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d544bff9-f18b-4b3e-9e9e-06d9481d98a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['author_synthetic'] = combined_df['author_synthetic'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42f156f2-b99a-4308-9117-64c61d27881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df1 = combined_df.merge(subred_cluster, how = 'outer', on = 'subreddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79459fcb-5de0-4f21-a59a-af595413625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df2 = combined_df1.merge(timezone_loc, how = 'outer', on = 'author_synthetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ed666c1-08c2-47f6-baab-e9051a69f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df2 = combined_df2.dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c1500f-bd99-465c-92ca-e85aabca31e1",
   "metadata": {},
   "source": [
    "## Export as Parquet File\n",
    "\n",
    "Now that all of the data merging is complete, you can export your dataframe as a `parquet` file, and when you want to read it in again in a separate notebook, you can use the `.read_parquet()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6160e23b-c983-4435-b3bf-d08346ff345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df2.to_parquet(DATA_DIR + '/merged_combined_file.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
